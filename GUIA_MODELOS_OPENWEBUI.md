# ü§ñ Gu√≠a Completa de Modelos para OpenWebUI

## ‚úÖ Estado Actual: MODELOS INSTALADOS

### üéâ Modelos Disponibles:

1. **Llama 3.2 1B** (1.3 GB) - Modelo ultraligero y r√°pido
2. **Llama 3.2 3B** (2.0 GB) - Modelo equilibrado y eficiente

### üîß Servicios Ejecut√°ndose:

- ‚úÖ **OpenWebUI**: Puerto 12000
- ‚úÖ **Ollama**: Puerto 11434
- ‚úÖ **Conexi√≥n**: Configurada autom√°ticamente

## üöÄ C√≥mo Usar los Modelos

### 1. Acceder a OpenWebUI:
- URL: https://work-1-pmibcoomalqbelgr.prod-runtime.all-hands.dev
- Crear cuenta de administrador (si no existe)
- Los modelos aparecer√°n autom√°ticamente en el selector

### 2. Seleccionar Modelo:
- En la interfaz, busca el selector de modelos
- Ver√°s: `llama3.2:1b` y `llama3.2:3b`
- Selecciona el que prefieras usar

## üì• Descargar M√°s Modelos

### Modelos Recomendados por Tama√±o:

#### üü¢ Ligeros (1-3GB) - R√°pidos:
```bash
ollama pull llama3.2:1b          # 1.3GB - Ultraligero
ollama pull llama3.2:3b          # 2.0GB - Ya instalado
ollama pull phi3:mini            # 2.3GB - Microsoft Phi-3
ollama pull gemma2:2b            # 1.6GB - Google Gemma 2
```

#### üü° Medianos (4-8GB) - Equilibrados:
```bash
ollama pull llama3.1:8b          # 4.7GB - Llama 3.1
ollama pull qwen2.5:7b           # 4.4GB - Multiling√ºe excelente
ollama pull mistral:7b           # 4.1GB - Mistral AI
ollama pull codellama:7b         # 3.8GB - Especializado en c√≥digo
```

#### üî¥ Grandes (10GB+) - M√°xima calidad:
```bash
ollama pull llama3.1:70b         # 40GB - Modelo premium
ollama pull qwen2.5:14b          # 8.2GB - Muy bueno
ollama pull mixtral:8x7b         # 26GB - Mixture of Experts
```

### Modelos Especializados:

#### üíª Para Programaci√≥n:
```bash
ollama pull codellama:7b         # C√≥digo general
ollama pull deepseek-coder:6.7b  # Excelente para c√≥digo
ollama pull starcoder2:3b        # Ligero para c√≥digo
```

#### üåç Multiling√ºes:
```bash
ollama pull qwen2.5:7b           # Excelente en espa√±ol
ollama pull aya:8b               # Multiling√ºe de Cohere
```

#### üî¨ Matem√°ticas y Ciencias:
```bash
ollama pull mathstral:7b         # Especializado en matem√°ticas
ollama pull llama3.1:8b          # Bueno en razonamiento
```

## üõ†Ô∏è Comandos √ötiles

### Gesti√≥n de Modelos:
```bash
# Listar modelos instalados
ollama list

# Descargar un modelo
ollama pull [nombre_modelo]

# Eliminar un modelo
ollama rm [nombre_modelo]

# Probar un modelo
ollama run [nombre_modelo] "Hola, ¬øc√≥mo est√°s?"

# Ver informaci√≥n de un modelo
ollama show [nombre_modelo]
```

### Gesti√≥n de Servicios:
```bash
# Verificar que Ollama est√° ejecut√°ndose
ps aux | grep ollama

# Iniciar Ollama (si no est√° ejecut√°ndose)
ollama serve &

# Verificar modelos disponibles via API
curl http://localhost:11434/api/tags
```

## üéØ Recomendaciones por Uso

### Para Empezar (Recursos Limitados):
- **llama3.2:1b** - Ya instalado, muy r√°pido
- **phi3:mini** - Alternativa ligera de Microsoft

### Para Uso General (Equilibrio):
- **llama3.2:3b** - Ya instalado, buen equilibrio
- **qwen2.5:7b** - Excelente multiling√ºe

### Para Programaci√≥n:
- **codellama:7b** - Especializado en c√≥digo
- **deepseek-coder:6.7b** - Muy bueno para desarrollo

### Para M√°xima Calidad (Si tienes recursos):
- **llama3.1:8b** - Excelente rendimiento general
- **qwen2.5:14b** - Premium multiling√ºe

## üîß Configuraci√≥n Avanzada

### Variables de Entorno para Ollama:
```bash
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MODELS=/path/to/models  # Cambiar ubicaci√≥n de modelos
export OLLAMA_NUM_PARALLEL=2          # Modelos en paralelo
export OLLAMA_MAX_LOADED_MODELS=3     # M√°ximo modelos cargados
```

### Configurar OpenWebUI para Ollama:
OpenWebUI detecta autom√°ticamente Ollama en `http://localhost:11434`

Si necesitas configurar manualmente:
1. Ve a Configuraci√≥n en OpenWebUI
2. Busca "Connections" o "Models"
3. A√±ade: `http://localhost:11434`

## üìä Comparaci√≥n de Modelos Instalados

| Modelo | Tama√±o | Velocidad | Calidad | Uso Recomendado |
|--------|--------|-----------|---------|-----------------|
| llama3.2:1b | 1.3GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Chat r√°pido, pruebas |
| llama3.2:3b | 2.0GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Uso general equilibrado |

## üö® Soluci√≥n de Problemas

### Modelo no aparece en OpenWebUI:
```bash
# Verificar que Ollama est√° ejecut√°ndose
curl http://localhost:11434/api/tags

# Reiniciar OpenWebUI si es necesario
pkill -f open-webui
# Luego reiniciar con el script
```

### Error de memoria:
- Usa modelos m√°s peque√±os (1b, 3b)
- Cierra otros programas
- Considera usar cuantizaci√≥n Q4 o Q8

### Modelo muy lento:
- Usa modelos m√°s peque√±os
- Verifica que no hay otros procesos pesados
- Considera usar GPU si est√° disponible

## üéâ ¬°Listo para Usar!

Tus modelos est√°n instalados y funcionando. Puedes:

1. **Acceder a OpenWebUI**: https://work-1-pmibcoomalqbelgr.prod-runtime.all-hands.dev
2. **Crear/Iniciar sesi√≥n** en tu cuenta
3. **Seleccionar modelo** en el selector
4. **¬°Comenzar a chatear!**

Los modelos `llama3.2:1b` y `llama3.2:3b` est√°n listos para usar inmediatamente.